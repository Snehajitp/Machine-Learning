{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree | Assignment\n"
      ],
      "metadata": {
        "id": "Lnuk7U78nkRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?**\n"
      ],
      "metadata": {
        "id": "43kV3WT5nkN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "A **Decision Tree** is a machine learning model that looks like a flowchart. It is used for classification problems. The tree splits the data into smaller groups based on different features.\n",
        "\n",
        "It works by asking a series of questions. For example: “Is age > 30?” Depending on the answer, it moves to the next branch. At each step, the tree chooses the feature that best separates the classes using measures like **Gini impurity** or **information gain**.\n",
        "\n",
        "This process continues until the tree reaches a **leaf node**, which gives the final class label.\n",
        "So basically, a Decision Tree keeps dividing the data based on conditions, and finally predicts the class by following the path from the root to a leaf.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ios16AXep58z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?**"
      ],
      "metadata": {
        "id": "c3XVbJUunkLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Gini Impurity and Entropy are impurity measures used to decide the quality of a split in a Decision Tree.\n",
        "\n",
        "Gini Impurity: Measures how often a randomly chosen sample would be misclassified.\n",
        "\n",
        "Gini = 0 means the node is pure (all samples in one class).\n",
        "\n",
        "Entropy: Measures the amount of randomness or disorder in the node.\n",
        "\n",
        "Entropy = 0 also means the node is pure.\n",
        "\n",
        "Impact on splits:\n",
        "A Decision Tree chooses the feature that reduces impurity the most.\n",
        "Lower Gini or lower Entropy after a split means better purity, so the tree selects that split to grow the tree."
      ],
      "metadata": {
        "id": "G7dThxfzp6lp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.**"
      ],
      "metadata": {
        "id": "AWsUJPaXnkIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Pre-Pruning:\n",
        "\n",
        "Pre-pruning stops the tree from growing too deep while it is being built.\n",
        "Examples: setting max depth, minimum samples to split, etc.\n",
        "Advantage: Saves time and prevents the tree from becoming too complex.\n",
        "\n",
        "Post-Pruning:\n",
        "\n",
        "Post-pruning allows the tree to grow fully first and then cuts off unnecessary branches afterward.\n",
        "Advantage: Usually gives better accuracy because the tree learns all patterns first and then removes only the weak splits.\n"
      ],
      "metadata": {
        "id": "gkI9MzfDp7dI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?**"
      ],
      "metadata": {
        "id": "PYsKgZKQnj-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Information Gain is a measure that tells us how much a feature helps in reducing uncertainty (entropy) when splitting the data in a Decision Tree.\n",
        "\n",
        "It is calculated as:\n",
        "\n",
        "Information Gain = Entropy before split – Entropy after split\n",
        "\n",
        "Information Gain helps the tree choose the best feature for splitting.\n",
        "A higher Information Gain means the split makes the data more pure, so the tree selects that feature to create a better and more accurate node."
      ],
      "metadata": {
        "id": "v2DyKC5pp8Bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "Dataset Info:\n",
        "\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV)."
      ],
      "metadata": {
        "id": "-JmGq0JRoQ44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "**Real-world applications of Decision Trees:**\n",
        "\n",
        "1. **Medical Diagnosis** – predicting diseases based on symptoms.\n",
        "2. **Finance** – loan approval, credit risk prediction.\n",
        "3. **Marketing** – identifying customer segments.\n",
        "4. **Manufacturing** – predicting equipment failure.\n",
        "5. **Classification problems** like the **Iris dataset** (classifying flower species).\n",
        "6. **Regression problems** like the **Boston Housing dataset** (predicting house prices).\n",
        "\n",
        "**Advantages of Decision Trees:**\n",
        "\n",
        "* Easy to understand and visualize.\n",
        "* Works with both classification and regression.\n",
        "* Requires little data preprocessing (no scaling needed).\n",
        "* Can handle both numerical and categorical features.\n",
        "\n",
        "**Limitations of Decision Trees:**\n",
        "\n",
        "* Can easily **overfit** if not pruned.\n",
        "* Small changes in data can change the entire tree (unstable).\n",
        "* Not as accurate as ensemble models like Random Forests.\n",
        "* May struggle with very complex datasets.\n"
      ],
      "metadata": {
        "id": "INq8usNep9pe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "Dq4kdlvwoULQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlNkts9Or1UI",
        "outputId": "5c40fd0b-d546-49a0-c801-3e1481ae5df1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "\n",
        "a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "tMC4-Vb3oSyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "pred_limited = tree_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, pred_limited)\n",
        "\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "pred_full = tree_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, pred_full)\n",
        "\n",
        "print(\"Accuracy with max_depth=3:\", accuracy_limited)\n",
        "print(\"Accuracy of fully-grown tree:\", accuracy_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6veCxE3sNcr",
        "outputId": "a97cb78a-bb01-4f05-93ed-f53517679053"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0\n",
            "Accuracy of fully-grown tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Load the Boston Housing Dataset\n",
        "\n",
        "● Train a Decision Tree Regressor\n",
        "\n",
        "● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "vys1np2Fo0xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vYYiU97sent",
        "outputId": "308bbcfb-d8d6-4039-fcc7-fef70eaae06c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280096503174904\n",
            "Feature Importances: [0.52345628 0.05213495 0.04941775 0.02497426 0.03220553 0.13901245\n",
            " 0.08999238 0.08880639]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "Y_H6HwJZpMdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(model, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z0VwFSGtJeP",
        "outputId": "d59723a5-f4a7-49d9-b4a4-b623301a5430"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n"
      ],
      "metadata": {
        "id": "Gg_QXdympYWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer —\n",
        "\n",
        "Below is a clear, practical procedure you could follow when building a Decision Tree to predict a disease from a large mixed dataset.\n",
        "\n",
        "\n",
        "\n",
        "### 1) Handle missing values\n",
        "\n",
        "1. **Understand the pattern:** Check how much is missing per feature and whether missingness is random (MCAR), related to other features (MAR), or informative (MNAR).\n",
        "2. **Remove only if justified:** Drop features with very high missing rate (e.g., > 50%) or rows if only a tiny fraction of data are missing.\n",
        "3. **Impute sensibly:**\n",
        "\n",
        "   * **Numerical:** median (robust) or mean; use KNN or iterative imputer if relationships are complex.\n",
        "   * **Categorical:** a new category like `\"Missing\"` or the mode; or use predictive imputation.\n",
        "4. **Keep a missing indicator:** For important features, add a boolean column `feature_is_missing` so model can learn missingness patterns.\n",
        "5. **Use the same imputer in production:** Fit imputers on training data and apply to test/production (use pipelines).\n",
        "\n",
        "\n",
        " ### 2) Encode categorical features\n",
        "\n",
        "1. **Low-cardinality categorical (few unique values):** Use **One-Hot Encoding**.\n",
        "2. **High-cardinality categorical:** Use **Target Encoding**, **Ordinal Encoding**, or embeddings (be careful of leakage—do target encoding inside CV).\n",
        "3. **Ordered categories:** Use **Ordinal Encoding** if order matters.\n",
        "4. **Trees & scaling:** Decision Trees don’t need feature scaling.\n",
        "5. **Use pipelines / ColumnTransformer** to apply different encodings to different columns and avoid leakage.\n",
        "\n",
        "\n",
        "### 3) Train a Decision Tree model\n",
        "\n",
        "1. **Train/test split:** Split (e.g., 70/30 or 80/20) or use nested CV for robust estimates.\n",
        "2. **Baseline model:** Train a simple DecisionTreeClassifier (default criteria Gini or Entropy).\n",
        "3. **Use class weights if imbalance:** `class_weight='balanced'` or oversample/undersample (SMOTE) inside CV.\n",
        "4. **Fit using a pipeline** that includes imputation and encoding to avoid leakage.\n",
        "\n",
        "\n",
        "\n",
        "### 4) Tune hyperparameters\n",
        "\n",
        "1. **Key hyperparameters to tune:** `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features`, `criterion` (gini/entropy), `ccp_alpha` (cost complexity pruning).\n",
        "2. **Search method:** GridSearchCV or RandomizedSearchCV with cross-validation (e.g., 5-fold). For speed, RandomizedSearch is good with many params.\n",
        "3. **Nested CV (if possible):** Use nested CV to get unbiased performance estimates when tuning.\n",
        "4. **Pruning:** Use `ccp_alpha` (cost-complexity pruning) to control overfitting.\n",
        "5. **Use scoring aligned with business goal:** e.g., maximize recall if missing a disease is costly, or maximize precision if false positives are expensive.\n",
        "\n",
        "\n",
        "### 5) Evaluate performance\n",
        "\n",
        "1. **Choose metrics suited to problem & cost of errors:**\n",
        "\n",
        "   * Classification: **Accuracy, Precision, Recall, F1-score, ROC-AUC, PR-AUC.**\n",
        "   * For imbalanced disease detection, emphasize **Recall (sensitivity)** and **ROC-AUC / PR-AUC**.\n",
        "2. **Confusion matrix:** Inspect TP, TN, FP, FN to understand types of errors.\n",
        "3. **Calibration:** Check probability calibration (especially important in clinical settings) — use calibration plots or `CalibratedClassifierCV`.\n",
        "4. **Threshold tuning:** Select decision threshold based on business trade-offs (e.g., choose threshold that achieves desired recall).\n",
        "5. **Explainability:** Report feature importances and use SHAP or LIME to explain individual predictions (critical in healthcare).\n",
        "6. **Robustness checks:** Test on holdout set, time-split validation, and subgroups (age, gender) to check fairness.\n",
        "7. **Monitoring:** Once deployed, monitor drift, performance, and data quality.\n",
        "\n",
        "\n",
        "### 6) Business value (real-world)\n",
        "\n",
        "* **Early detection:** Identify high-risk patients earlier so clinicians can intervene sooner.\n",
        "* **Prioritize resources:** Triage tests, specialist appointments, and follow-ups for likely positive cases.\n",
        "* **Cost reduction:** Reduce unnecessary expensive tests by screening patients first.\n",
        "* **Improved outcomes:** Early treatment can reduce morbidity, hospital stays, and downstream costs.\n",
        "* **Operational efficiency:** Automate part of screening workflows, freeing clinician time.\n",
        "* **Interpretability helps adoption:** Decision Trees are interpretable—easier to explain to clinicians and regulators.\n",
        "\n",
        "\n",
        "### 7) Practical & ethical considerations (important in healthcare)\n",
        "\n",
        "* **Avoid data leakage** (e.g., features recorded after diagnosis).\n",
        "* **Bias & fairness:** Check model performance across demographic groups to avoid harming vulnerable populations.\n",
        "* **Privacy & compliance:** Ensure HIPAA/GDPR-like safeguards for patient data.\n",
        "* **Human-in-the-loop:** Use model as decision support, not an absolute decision-maker.\n",
        "* **Clinical validation:** Validate prospectively and with clinicians before deployment.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yi6u7s06toXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 3, 4, 5, 10],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(model, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WhOACEqtoA-",
        "outputId": "aa0f5631-0656-40c3-8d5e-33e086754feb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Accuracy: 1.0\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_6nen54UqBdL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2XSsXH7nH5Z"
      },
      "outputs": [],
      "source": []
    }
  ]
}